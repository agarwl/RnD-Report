# RnD Project Report

Recently, word embeddings have been exceptionally successful in many NLP
tasks. In fact, in many NLP architectures, they have almost completely replaced
traditional distributional features such as Brown clusters and LSA
features.

Semantic relations between word embeddings seem nothing short of magical
to the uninitiated and Deep Learning NLP talks frequently prelude with
the notorious king − man + woman ≈ queen slide, word embeddings are
possibly the primary reason for NLP’s breakout recently.

There has already been a lot of work in the quest for the best embedding
models. Lately, there has been lot of papers explaining why such embedding
models work and need for better formulation of such models. This work
summarizes my study on word embeddings, embeddings for knowledge graphs
and entity embeddings advised by [Prof. Soumen Chakrabarti](https://www.cse.iitb.ac.in/~soumen/).
